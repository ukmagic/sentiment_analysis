# -*- coding: utf-8 -*-
"""Upwork-Review-Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_4G7Yen02dtovcPWPa7-JpcWZn2FNBw

<h2 align=center> Toxic Comments Classification using 1D CNN with Keras</h2>

### Task 1: Import Packages and Functions
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing import text, sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.layers import Embedding,LSTM, Bidirectional
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
import re
from keras.optimizers import Adam
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
print(tf.__version__)

"""### Task 2: Load and Explore Data"""

# Load data
review=pd.read_csv('/content/reviews.csv',header=None,names=['reviews'])
meta=pd.read_csv('/content/metadata.csv',usecols=['SENTIMENT','TOPIC'])
meta.columns = ['sentiment', 'topic']
meta

task1_df = pd.DataFrame([review.reviews, meta.sentiment]).transpose()
#task1_df=task1_df.dropna(subset=['topic'])
task1_df['sentiment'] = (task1_df['sentiment'] == 'pos').astype(int)
task1_df

x= task1_df['reviews'].values
cv = CountVectorizer(min_df=0.05, max_df=0.95,stop_words='english') 
X = cv.fit_transform(x)

x= task1_df['reviews'].values

"""### Task 3- View few positive comments

"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
comments = task1_df['reviews'].loc[task1_df['sentiment']==1].values
wordcloud = WordCloud(
    width = 640,
    height = 640,
    background_color = 'black',
    stopwords = STOPWORDS).generate(str(comments))
fig = plt.figure(
    figsize = (12, 8),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

y=task1_df['sentiment'].values.astype(int)
task1_df['sentiment'].value_counts()

# Plot frequency of toxic comments
task1_df['sentiment'].value_counts().plot(kind='bar', title='Distribution of Positive and Negative Reviews')

"""### Task 4: Data Prep â€” Tokenize and Pad Text Data"""

max_features=20000
max_text_len=400
embedding_dim = 100
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
filters=128
kernel_size=5
hidden_dims=24

tokenizer=text.Tokenizer(max_features, oov_token=oov_tok)
tokenizer.fit_on_texts(list(x))
word_index = tokenizer.word_index
tokenized=tokenizer.texts_to_sequences(x)
seq=sequence.pad_sequences(tokenized,maxlen=max_text_len, padding=padding_type, truncating=trunc_type)

"""### Task 5: Create the Embedding Layer"""

model = Sequential()
model.add(Embedding(max_features, embedding_dim, input_length=max_text_len))
model.add(Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.2)))
model.add(Dense(256, activation='relu'))
model.add(Dense(512, activation='relu'))

model.add(Dropout(0.50))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])

"""### Task 7: Train Model"""

X_train,X_val,y_train,y_val=train_test_split(seq,y,test_size=0.5,random_state=0)

batch_size=64
early=EarlyStopping(monitor='val_loss',patience=6,mode='min')
checkpoint = ModelCheckpoint("model.h5",
                             monitor="val_accuracy",
                             mode="max",
                             save_best_only = True,
                             verbose=1)

reduce_lor = ReduceLROnPlateau(monitor = 'val_accuracy', 
                               mode='max',
                          min_delta = 0, 
                          patience = 3,
                          verbose = 1)
hist=model.fit(X_train,y_train,batch_size=batch_size,epochs=30,validation_data=(X_val,y_val),callbacks=[checkpoint,early,reduce_lor])

from keras.models import load_model
model=load_model('model.h5')
model.save('sentiment_analysis.h5')

"""### Task-8 Plot accuracy vs epoch and loss vs epoch"""

epochs = range(len(hist.history['accuracy']))
plt.plot(epochs,hist.history['accuracy'],label='training')
plt.plot(epochs,hist.history['val_accuracy'],label='validation')
plt.xlabel("Epochs")
plt.ylabel('Accuracy')
plt.xticks(range(len(hist.history['accuracy'])))
plt.yticks([0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1])
plt.title('Accuracy vs Epoch')
plt.legend()
plt.show()

epochs = range(len(hist.history['accuracy']))
plt.plot(epochs,hist.history['loss'],label='Training')
plt.plot(epochs,hist.history['val_loss'],label='Val')
plt.xlabel("Epochs")
plt.ylabel('Loss')
plt.xticks(range(len(hist.history['accuracy'])))
plt.title('Loss vs Epoch')
plt.legend()
plt.show()

"""### Task 9: Evaluate Model"""

model.evaluate(X_val,y_val)

"""### Task 10- Make predictions"""

s=input("Enter your string:  ")
s=np.array([s])
tokenized=tokenizer.texts_to_sequences(s)
X_test=sequence.pad_sequences(tokenized,maxlen=max_text_len, padding=padding_type, truncating=trunc_type)
y_test=model.predict(X_test,verbose=1,batch_size=128)
val=['neg' if x < .5 else 'pos' for x in y_test]
print(s[0],'-',val[0])

