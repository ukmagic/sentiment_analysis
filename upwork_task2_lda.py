# -*- coding: utf-8 -*-
"""Upwork_Task2_LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zcx7vwLUsqV4waAaAEjJXZmjFuo8J1FP

## Step 1: Load the dataset
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing import text, sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.layers import Embedding,LSTM, Bidirectional
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
import re
from keras.optimizers import Adam
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
print(tf.__version__)

# Load data
task2_df=pd.read_csv('/content/reviews.csv',header=None,names=['reviews'])
task2_df

"""## Step 2: Data Preprocessing ##

We will perform the following steps:

* **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.
* Words that have fewer than 3 characters are removed.
* All **stopwords** are removed.
* Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.
* Words are **stemmed** - words are reduced to their root form.
* **Bigrams** and **TfIdf** is implemented
* **Limiting frequency** is used

"""

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
from gensim import corpora, models

np.random.seed(400)

import nltk 
from nltk import word_tokenize 
from nltk.util import ngrams
nltk.download('punkt')
nltk.download('wordnet')

"""## Step 3: Perform Topic Modelling"""

def main(task2_df,num_classes,stem):

    def lemmatize_stemming(text,stem):
        stemmer = SnowballStemmer("english")
        if stem:
            return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
        else:
            return WordNetLemmatizer().lemmatize(text, pos='v')


    # Tokenize and lemmatize
    def preprocess(text,stem):
        result=[]
        for token in gensim.utils.simple_preprocess(text) :
            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
                result.append(lemmatize_stemming(token,stem))
                
        return result
    def get_ngrams(text):
        big=[]
        token = nltk.word_tokenize(text)
        bigram = list(ngrams(token, 2))
        for i in bigram:
            big.append(i[0]+' '+i[1])
        return [big]

    processed_docs = []
    for doc in task2_df.reviews:
        processed_docs.append(preprocess(doc,stem))
        s=' '.join(word for i in processed_docs for word in i)    
        bigram=get_ngrams(s)

    dictionary = gensim.corpora.Dictionary(processed_docs)
    dictionary.filter_extremes(no_below=5, no_above=0.95, keep_n= 1000)
    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

    tfidf = models.TfidfModel(bow_corpus)
    corpus_tfidf = tfidf[bow_corpus]

    lda_model =  gensim.models.LdaMulticore(corpus_tfidf, 
                                    num_topics = num_classes, 
                                    id2word = dictionary,                                    
                                    passes = 10,
                                    workers = 2)
    
    for idx, topic in lda_model.print_topics(-1):
        print("Topic: {} \nWords: {}".format(idx, topic))
        print("\n")
    return lda_model

"""Case 1: 3 Topics"""

lda_model=main(task2_df,3,True)

"""Case 2: 4 Topics"""

lda_model=main(task2_df,4,True)

"""Case 3: 3 Topics and Without Stemming"""

lda_model=main(task2_df,3,False)

"""## Step 4: Testing model on unseen document ##"""

unseen_document = '1st half: excellent Interval: ðŸ‘Œ 2nd half: just avg Climax twist:\n ðŸ‘ŒðŸ‘ŒðŸ‘Œ Cast: Nithin, sai chand, Rakul did excellent. Nithiin acting'
print(unseen_document)

# Data preprocessing step for the unseen document
bow_vector = dictionary.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
    print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))

