{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Upwork_Task2_LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vcewZf1ruC"
      },
      "source": [
        "## Step 1: Load the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s_aTc-r5NSt",
        "outputId": "e9af4468-379c-4b5a-e12b-c0420b1611e2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding,LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "A2lpR39z1ruG",
        "outputId": "bc4e43a8-44ce-41b1-fea6-5d38393cd84f"
      },
      "source": [
        "# Load data\n",
        "task2_df=pd.read_csv('/content/reviews.csv',header=None,names=['reviews'])\n",
        "task2_df"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i did not enjoy the japanese restaurant on par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>batman v superman oh my god what an abysmal mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i went to a restaurant down the street and had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the star wars the rise of skywalker 2 out of 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it was a very engaging book that was very easy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>bad moms is a hyperbolic romantic comedy that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>i didn t understand the point of once upon a t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>recently my wife and i shared a meal together ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>it is not difficult to find pio pio in the lis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>last christmas was an awful attempt at a holid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1244 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                reviews\n",
              "0     i did not enjoy the japanese restaurant on par...\n",
              "1     batman v superman oh my god what an abysmal mo...\n",
              "2     i went to a restaurant down the street and had...\n",
              "3     the star wars the rise of skywalker 2 out of 5...\n",
              "4     it was a very engaging book that was very easy...\n",
              "...                                                 ...\n",
              "1239  bad moms is a hyperbolic romantic comedy that ...\n",
              "1240  i didn t understand the point of once upon a t...\n",
              "1241  recently my wife and i shared a meal together ...\n",
              "1242  it is not difficult to find pio pio in the lis...\n",
              "1243  last christmas was an awful attempt at a holid...\n",
              "\n",
              "[1244 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sljzLQNK1ruS"
      },
      "source": [
        "## Step 2: Data Preprocessing ##\n",
        "\n",
        "We will perform the following steps:\n",
        "\n",
        "* **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
        "* Words that have fewer than 3 characters are removed.\n",
        "* All **stopwords** are removed.\n",
        "* Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
        "* Words are **stemmed** - words are reduced to their root form.\n",
        "* **Bigrams** and **TfIdf** is implemented\n",
        "* **Limiting frequency** is used\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSZ-Dp7G1ruT"
      },
      "source": [
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "from gensim import corpora, models\n",
        "\n",
        "np.random.seed(400)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN3QVQlj1ruV",
        "outputId": "d528cf8b-a1ec-42c5-ec24-480d6e2dc866"
      },
      "source": [
        "import nltk \n",
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUKxm6mVDKx"
      },
      "source": [
        "## Step 3: Perform Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xs6kJ0h1ruc"
      },
      "source": [
        "def main(task2_df,num_classes,stem):\n",
        "\n",
        "    def lemmatize_stemming(text,stem):\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "        if stem:\n",
        "            return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "        else:\n",
        "            return WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "\n",
        "\n",
        "    # Tokenize and lemmatize\n",
        "    def preprocess(text,stem):\n",
        "        result=[]\n",
        "        for token in gensim.utils.simple_preprocess(text) :\n",
        "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "                result.append(lemmatize_stemming(token,stem))\n",
        "                \n",
        "        return result\n",
        "    def get_ngrams(text):\n",
        "        big=[]\n",
        "        token = nltk.word_tokenize(text)\n",
        "        bigram = list(ngrams(token, 2))\n",
        "        for i in bigram:\n",
        "            big.append(i[0]+' '+i[1])\n",
        "        return [big]\n",
        "\n",
        "    processed_docs = []\n",
        "    for doc in task2_df.reviews:\n",
        "        processed_docs.append(preprocess(doc,stem))\n",
        "        s=' '.join(word for i in processed_docs for word in i)    \n",
        "        bigram=get_ngrams(s)\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.95, keep_n= 1000)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "    tfidf = models.TfidfModel(bow_corpus)\n",
        "    corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "    lda_model =  gensim.models.LdaMulticore(corpus_tfidf, \n",
        "                                    num_topics = num_classes, \n",
        "                                    id2word = dictionary,                                    \n",
        "                                    passes = 10,\n",
        "                                    workers = 2)\n",
        "    \n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "        print(\"\\n\")\n",
        "    return lda_model"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFnJr4371ruf"
      },
      "source": [
        "Case 1: 3 Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9EObBm1rug",
        "outputId": "e708661d-4139-4ffa-fb4a-287f5f3c757a"
      },
      "source": [
        "lda_model=main(task2_df,3,True)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.017*\"book\" + 0.009*\"stori\" + 0.009*\"read\" + 0.006*\"human\" + 0.005*\"histori\" + 0.005*\"author\" + 0.005*\"world\" + 0.005*\"charact\" + 0.005*\"write\" + 0.005*\"season\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.011*\"restaur\" + 0.011*\"food\" + 0.007*\"experi\" + 0.007*\"servic\" + 0.006*\"great\" + 0.006*\"place\" + 0.006*\"good\" + 0.005*\"dish\" + 0.005*\"time\" + 0.005*\"order\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.028*\"movi\" + 0.011*\"watch\" + 0.011*\"film\" + 0.009*\"plot\" + 0.008*\"charact\" + 0.007*\"stori\" + 0.007*\"act\" + 0.005*\"action\" + 0.005*\"storylin\" + 0.005*\"comedi\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.ldamulticore.LdaMulticore at 0x7f20d9e98410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrAvXnpVOzN"
      },
      "source": [
        "Case 2: 4 Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t2E_N3MUKSu",
        "outputId": "a595e5ee-c0f7-45bf-82ad-1a49d075617a"
      },
      "source": [
        "lda_model=main(task2_df,4,True)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.015*\"movi\" + 0.010*\"book\" + 0.008*\"charact\" + 0.008*\"stori\" + 0.007*\"film\" + 0.006*\"watch\" + 0.006*\"plot\" + 0.005*\"seri\" + 0.005*\"action\" + 0.005*\"think\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.010*\"movi\" + 0.008*\"ramen\" + 0.006*\"sapien\" + 0.006*\"month\" + 0.005*\"harari\" + 0.005*\"watch\" + 0.005*\"act\" + 0.005*\"time\" + 0.005*\"human\" + 0.004*\"save\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.014*\"restaur\" + 0.014*\"food\" + 0.008*\"servic\" + 0.008*\"experi\" + 0.007*\"dish\" + 0.007*\"place\" + 0.007*\"order\" + 0.006*\"great\" + 0.006*\"price\" + 0.006*\"go\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.016*\"movi\" + 0.013*\"book\" + 0.010*\"stori\" + 0.008*\"read\" + 0.007*\"film\" + 0.007*\"life\" + 0.006*\"watch\" + 0.006*\"charact\" + 0.005*\"plot\" + 0.005*\"think\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3EaqFAkVTPt"
      },
      "source": [
        "Case 3: 3 Topics and Without Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_lEMmTZUJ_U",
        "outputId": "c5fcd928-31bf-41aa-d376-f6f1a589e23d"
      },
      "source": [
        "lda_model=main(task2_df,3,False)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.017*\"book\" + 0.010*\"read\" + 0.006*\"write\" + 0.006*\"story\" + 0.006*\"history\" + 0.006*\"life\" + 0.005*\"author\" + 0.005*\"season\" + 0.005*\"world\" + 0.004*\"work\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.022*\"movie\" + 0.010*\"story\" + 0.009*\"film\" + 0.009*\"watch\" + 0.008*\"book\" + 0.008*\"plot\" + 0.008*\"character\" + 0.006*\"movies\" + 0.006*\"life\" + 0.006*\"think\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.013*\"food\" + 0.013*\"restaurant\" + 0.008*\"service\" + 0.007*\"experience\" + 0.007*\"place\" + 0.007*\"dish\" + 0.006*\"great\" + 0.006*\"order\" + 0.006*\"good\" + 0.006*\"price\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEj-lkAr1ru-"
      },
      "source": [
        "## Step 4: Testing model on unseen document ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF5zGI7_1ru_",
        "outputId": "f303f7af-534f-4eaa-e090-b4819fd4f84b"
      },
      "source": [
        "unseen_document = '1st half: excellent Interval: ðŸ‘Œ 2nd half: just avg Climax twist:\\n ðŸ‘ŒðŸ‘ŒðŸ‘Œ Cast: Nithin, sai chand, Rakul did excellent. Nithiin acting'\n",
        "print(unseen_document)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st half: excellent Interval: ðŸ‘Œ 2nd half: just avg Climax twist:\n",
            " ðŸ‘ŒðŸ‘ŒðŸ‘Œ Cast: Nithin, sai chand, Rakul did excellent. Nithiin acting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnqmN2dn1rvA",
        "outputId": "164d07f8-493f-4dc3-bc15-00ba879e9a35"
      },
      "source": [
        "# Data preprocessing step for the unseen document\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.7135390639305115\t Topic: 0.022*\"movie\" + 0.010*\"story\" + 0.009*\"film\" + 0.009*\"watch\" + 0.008*\"book\"\n",
            "Score: 0.2437753677368164\t Topic: 0.013*\"food\" + 0.013*\"restaurant\" + 0.008*\"service\" + 0.007*\"experience\" + 0.007*\"place\"\n",
            "Score: 0.042685553431510925\t Topic: 0.017*\"book\" + 0.010*\"read\" + 0.006*\"write\" + 0.006*\"story\" + 0.006*\"history\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2OKLaalXsVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}